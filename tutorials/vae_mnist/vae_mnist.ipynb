{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41344bb3",
   "metadata": {
    "papermill": {
     "duration": 0.004241,
     "end_time": "2026-01-08T16:52:38.827967",
     "exception": false,
     "start_time": "2026-01-08T16:52:38.823726",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## VAE MNIST example: BO in a latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3736d",
   "metadata": {
    "papermill": {
     "duration": 0.010658,
     "end_time": "2026-01-08T16:52:38.859301",
     "exception": false,
     "start_time": "2026-01-08T16:52:38.848643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this tutorial, we use the MNIST dataset and some standard PyTorch examples to show a synthetic problem where the input to the objective function is a `28 x 28` image. The main idea is to train a [variational auto-encoder (VAE)](https://arxiv.org/abs/1312.6114) on the MNIST dataset and run Bayesian Optimization in the latent space. We also refer readers to [this tutorial](http://krasserm.github.io/2018/04/07/latent-space-optimization/), which discusses [the method](https://arxiv.org/abs/1610.02415) of jointly training a VAE with a predictor (e.g., classifier), and shows a similar tutorial for the MNIST setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b2a298",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T16:52:38.879062Z",
     "iopub.status.busy": "2026-01-08T16:52:38.878763Z",
     "iopub.status.idle": "2026-01-08T16:52:38.892290Z",
     "shell.execute_reply": "2026-01-08T16:52:38.891743Z"
    },
    "papermill": {
     "duration": 0.025207,
     "end_time": "2026-01-08T16:52:38.893780",
     "exception": false,
     "start_time": "2026-01-08T16:52:38.868573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install dependencies if we are running in colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    %pip install botorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe8aef46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T16:52:38.910032Z",
     "iopub.status.busy": "2026-01-08T16:52:38.909762Z",
     "iopub.status.idle": "2026-01-08T16:52:46.239829Z",
     "shell.execute_reply": "2026-01-08T16:52:46.239066Z"
    },
    "papermill": {
     "duration": 7.340054,
     "end_time": "2026-01-08T16:52:46.242246",
     "exception": false,
     "start_time": "2026-01-08T16:52:38.902192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets  # transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa4c323",
   "metadata": {
    "papermill": {
     "duration": 0.017987,
     "end_time": "2026-01-08T16:52:46.277507",
     "exception": false,
     "start_time": "2026-01-08T16:52:46.259520",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Problem setup\n",
    "\n",
    "Let's first define our synthetic expensive-to-evaluate objective function. We assume that it takes the following form:\n",
    "\n",
    "$$\\text{image} \\longrightarrow \\text{image classifier} \\longrightarrow \\text{scoring function} \n",
    "\\longrightarrow \\text{score}.$$\n",
    "\n",
    "The classifier is a convolutional neural network (CNN) trained using the architecture of the [PyTorch CNN example](https://github.com/pytorch/examples/tree/master/mnist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "592b1506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T16:52:46.306603Z",
     "iopub.status.busy": "2026-01-08T16:52:46.305789Z",
     "iopub.status.idle": "2026-01-08T16:52:46.313750Z",
     "shell.execute_reply": "2026-01-08T16:52:46.313063Z"
    },
    "papermill": {
     "duration": 0.025704,
     "end_time": "2026-01-08T16:52:46.315276",
     "exception": false,
     "start_time": "2026-01-08T16:52:46.289572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6293a001",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T16:52:46.341439Z",
     "iopub.status.busy": "2026-01-08T16:52:46.340947Z",
     "iopub.status.idle": "2026-01-08T16:52:46.345861Z",
     "shell.execute_reply": "2026-01-08T16:52:46.345258Z"
    },
    "papermill": {
     "duration": 0.019258,
     "end_time": "2026-01-08T16:52:46.347297",
     "exception": false,
     "start_time": "2026-01-08T16:52:46.328039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pretrained_dir() -> str:\n",
    "    \"\"\"\n",
    "    Get the directory of pretrained models, which are in the BoTorch repo.\n",
    "\n",
    "    Returns the location specified by PRETRAINED_LOCATION if that env\n",
    "    var is set; otherwise checks if we are in a likely part of the BoTorch\n",
    "    repo (botorch/botorch or botorch/tutorials) and returns the right path.\n",
    "    \"\"\"\n",
    "    if \"PRETRAINED_LOCATION\" in os.environ.keys():\n",
    "        return os.environ[\"PRETRAINED_LOCATION\"]\n",
    "    cwd = os.getcwd()\n",
    "    folder = os.path.basename(cwd)\n",
    "    # automated tests run from botorch folder\n",
    "    if folder == \"botorch\":\n",
    "        return os.path.join(cwd, \"tutorials/pretrained_models/\")\n",
    "    # typical case (running from tutorial folder)\n",
    "    elif folder == \"tutorials\":\n",
    "        return os.path.join(cwd, \"pretrained_models/\")\n",
    "    raise FileNotFoundError(\"Could not figure out location of pretrained models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a83486",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T16:52:46.365298Z",
     "iopub.status.busy": "2026-01-08T16:52:46.365120Z",
     "iopub.status.idle": "2026-01-08T16:52:46.382625Z",
     "shell.execute_reply": "2026-01-08T16:52:46.382280Z"
    },
    "papermill": {
     "duration": 0.025021,
     "end_time": "2026-01-08T16:52:46.383672",
     "exception": false,
     "start_time": "2026-01-08T16:52:46.358651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_weights_path = os.path.join(get_pretrained_dir(), \"mnist_cnn.pt\")\n",
    "cnn_model = Net().to(dtype=dtype, device=device)\n",
    "cnn_state_dict = torch.load(cnn_weights_path, map_location=device, weights_only=True)\n",
    "cnn_model.load_state_dict(cnn_state_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c5a82f",
   "metadata": {
    "papermill": {
     "duration": 0.005483,
     "end_time": "2026-01-08T16:52:46.395116",
     "exception": false,
     "start_time": "2026-01-08T16:52:46.389633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Our VAE model follows the [PyTorch VAE example](https://github.com/pytorch/examples/tree/master/vae), except that we use the same data transform from the CNN tutorial for consistency. We then instantiate the model and again load a pre-trained model. To train these models, we refer readers to the PyTorch Github repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c773754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T16:52:46.407839Z",
     "iopub.status.busy": "2026-01-08T16:52:46.407659Z",
     "iopub.status.idle": "2026-01-08T16:52:46.421295Z",
     "shell.execute_reply": "2026-01-08T16:52:46.420937Z"
    },
    "papermill": {
     "duration": 0.02099,
     "end_time": "2026-01-08T16:52:46.422270",
     "exception": false,
     "start_time": "2026-01-08T16:52:46.401280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "vae_weights_path = os.path.join(get_pretrained_dir(), \"mnist_vae.pt\")\n",
    "vae_model = VAE().to(dtype=dtype, device=device)\n",
    "vae_state_dict = torch.load(vae_weights_path, map_location=device, weights_only=True)\n",
    "vae_model.load_state_dict(vae_state_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eada0264",
   "metadata": {
    "papermill": {
     "duration": 0.006245,
     "end_time": "2026-01-08T16:52:46.435571",
     "exception": false,
     "start_time": "2026-01-08T16:52:46.429326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We now define the scoring function that maps digits to scores. The function below prefers the digit '3'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f58c68d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T16:52:46.449238Z",
     "iopub.status.busy": "2026-01-08T16:52:46.449059Z",
     "iopub.status.idle": "2026-01-08T16:52:46.451403Z",
     "shell.execute_reply": "2026-01-08T16:52:46.451040Z"
    },
    "papermill": {
     "duration": 0.010425,
     "end_time": "2026-01-08T16:52:46.452404",
     "exception": false,
     "start_time": "2026-01-08T16:52:46.441979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score(y):\n",
    "    \"\"\"Returns a 'score' for each digit from 0 to 9. It is modeled as a squared exponential\n",
    "    centered at the digit '3'.\n",
    "    \"\"\"\n",
    "    return torch.exp(-2 * (y - 3) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae132cc9",
   "metadata": {
    "papermill": {
     "duration": 0.01028,
     "end_time": "2026-01-08T16:52:46.471949",
     "exception": false,
     "start_time": "2026-01-08T16:52:46.461669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Given the scoring function, we can now write our overall objective, which as discussed above, starts with an image and outputs a score. Let's say the objective computes the expected score given the probabilities from the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9979cf58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T16:52:46.485280Z",
     "iopub.status.busy": "2026-01-08T16:52:46.485100Z",
     "iopub.status.idle": "2026-01-08T16:52:46.487775Z",
     "shell.execute_reply": "2026-01-08T16:52:46.487487Z"
    },
    "papermill": {
     "duration": 0.009701,
     "end_time": "2026-01-08T16:52:46.488581",
     "exception": false,
     "start_time": "2026-01-08T16:52:46.478880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score_image(x):\n",
    "    \"\"\"The input x is an image and an expected score\n",
    "    based on the CNN classifier and the scoring\n",
    "    function is returned.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        probs = torch.exp(cnn_model(x))  # b x 10\n",
    "        scores = score(\n",
    "            torch.arange(10, device=device, dtype=dtype)\n",
    "        ).expand(probs.shape)\n",
    "    return (probs * scores).sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a32437",
   "metadata": {
    "papermill": {
     "duration": 0.018419,
     "end_time": "2026-01-08T16:52:46.515539",
     "exception": false,
     "start_time": "2026-01-08T16:52:46.497120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, we define a helper function `decode` that takes as input the parameters `mu` and `logvar` of the variational distribution and performs reparameterization and the decoding. We use batched Bayesian optimization to search over the parameters `mu` and `logvar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bb981b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T16:52:46.527517Z",
     "iopub.status.busy": "2026-01-08T16:52:46.527343Z",
     "iopub.status.idle": "2026-01-08T16:52:46.529777Z",
     "shell.execute_reply": "2026-01-08T16:52:46.529415Z"
    },
    "papermill": {
     "duration": 0.009258,
     "end_time": "2026-01-08T16:52:46.530675",
     "exception": false,
     "start_time": "2026-01-08T16:52:46.521417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode(train_x):\n",
    "    with torch.no_grad():\n",
    "        decoded = vae_model.decode(train_x)\n",
    "    return decoded.view(train_x.shape[0], 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971c556",
   "metadata": {
    "papermill": {
     "duration": 0.00464,
     "end_time": "2026-01-08T16:52:46.540282",
     "exception": false,
     "start_time": "2026-01-08T16:52:46.535642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Model initialization and initial random batch\n",
    "\n",
    "We use a `SingleTaskGP` to model the score of an image generated by a latent representation. The model is initialized with points drawn from $[-6, 6]^{20}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4991f874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T16:52:46.549770Z",
     "iopub.status.busy": "2026-01-08T16:52:46.549559Z",
     "iopub.status.idle": "2026-01-08T16:52:47.547603Z",
     "shell.execute_reply": "2026-01-08T16:52:47.547237Z"
    },
    "papermill": {
     "duration": 1.003999,
     "end_time": "2026-01-08T16:52:47.548533",
     "exception": false,
     "start_time": "2026-01-08T16:52:46.544534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[KeOps] Warning : There were warnings or errors :\n",
      "/bin/sh: brew: command not found\n",
      "\n",
      "[KeOps] Warning : CUDA libraries not found or could not be loaded; Switching to CPU only.\n",
      "\n",
      "[KeOps] Warning : There were warnings or errors :\n",
      "/bin/sh: brew: command not found\n",
      "\n",
      "[KeOps] Warning : OpenMP library not found, it must be downloaded through Homebrew for apple Silicon chips\n",
      "[KeOps] Warning : OpenMP support is not available. Disabling OpenMP.\n"
     ]
    }
   ],
   "source": [
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "from botorch.models.transforms import Normalize\n",
    "\n",
    "d = 20\n",
    "bounds = torch.tensor([[-6.0] * d, [6.0] * d], device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "def gen_initial_data(n=5):\n",
    "    # generate training data\n",
    "    train_x = unnormalize(\n",
    "        torch.rand(n, d, device=device, dtype=dtype),\n",
    "        bounds=bounds\n",
    "    )\n",
    "    train_obj = score_image(decode(train_x)).unsqueeze(-1)\n",
    "    best_observed_value = train_obj.max().item()\n",
    "    return train_x, train_obj, best_observed_value\n",
    "\n",
    "\n",
    "def get_fitted_model(train_x, train_obj, state_dict=None):\n",
    "    # initialize and fit model\n",
    "    model = SingleTaskGP(\n",
    "        train_X=normalize(train_x, bounds),\n",
    "        train_Y=train_obj,\n",
    "    )\n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    mll.to(train_x)\n",
    "    fit_gpytorch_mll(mll)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09be0f2",
   "metadata": {
    "papermill": {
     "duration": 0.011382,
     "end_time": "2026-01-08T16:52:47.571569",
     "exception": false,
     "start_time": "2026-01-08T16:52:47.560187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Define a helper function that performs the essential BO step\n",
    "The helper function below takes an acquisition function as an argument, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. For this example, we'll use a small batch of $q=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49dd233e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T16:52:47.589383Z",
     "iopub.status.busy": "2026-01-08T16:52:47.589103Z",
     "iopub.status.idle": "2026-01-08T16:52:47.591850Z",
     "shell.execute_reply": "2026-01-08T16:52:47.591600Z"
    },
    "papermill": {
     "duration": 0.011499,
     "end_time": "2026-01-08T16:52:47.592635",
     "exception": false,
     "start_time": "2026-01-08T16:52:47.581136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "\n",
    "BATCH_SIZE = 3 if not SMOKE_TEST else 2\n",
    "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
    "RAW_SAMPLES = 256 if not SMOKE_TEST else 4\n",
    "\n",
    "\n",
    "def optimize_acqf_and_get_observation(acq_func):\n",
    "    \"\"\"Optimizes the acquisition function, and returns a\n",
    "    new candidate and a noisy observation\"\"\"\n",
    "\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=torch.stack(\n",
    "            [\n",
    "                torch.zeros(d, dtype=dtype, device=device),\n",
    "                torch.ones(d, dtype=dtype, device=device),\n",
    "            ]\n",
    "        ),\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,\n",
    "    )\n",
    "\n",
    "    # observe new values\n",
    "    new_x = unnormalize(candidates.detach(), bounds=bounds)\n",
    "    new_obj = score_image(decode(new_x)).unsqueeze(-1)\n",
    "    return new_x, new_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ba705",
   "metadata": {
    "papermill": {
     "duration": 0.005842,
     "end_time": "2026-01-08T16:52:47.604852",
     "exception": false,
     "start_time": "2026-01-08T16:52:47.599010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Perform Bayesian Optimization loop with qEI\n",
    "The Bayesian optimization \"loop\" for a batch size of $q$ simply iterates the following steps: (1) given a surrogate model, choose a batch of points $\\{x_1, x_2, \\ldots x_q\\}$, (2) observe $f(x)$ for each $x$ in the batch, and (3) update the surrogate model. We run `N_BATCH=75` iterations. The acquisition function is approximated using `MC_SAMPLES=2048` samples. We also initialize the model with 5 randomly drawn points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84d943fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T16:52:47.614861Z",
     "iopub.status.busy": "2026-01-08T16:52:47.614656Z",
     "iopub.status.idle": "2026-01-08T16:52:47.632568Z",
     "shell.execute_reply": "2026-01-08T16:52:47.632238Z"
    },
    "papermill": {
     "duration": 0.023203,
     "end_time": "2026-01-08T16:52:47.633466",
     "exception": false,
     "start_time": "2026-01-08T16:52:47.610263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "N_BATCH = 25 if not SMOKE_TEST else 3\n",
    "best_observed = []\n",
    "\n",
    "# call helper function to initialize model\n",
    "train_x, train_obj, best_value = gen_initial_data(n=5)\n",
    "best_observed.append(best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4373135",
   "metadata": {
    "papermill": {
     "duration": 0.004006,
     "end_time": "2026-01-08T16:52:47.647229",
     "exception": false,
     "start_time": "2026-01-08T16:52:47.643223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We are now ready to run the BO loop (this make take a few minutes, depending on your machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96e0b92a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T16:52:47.656782Z",
     "iopub.status.busy": "2026-01-08T16:52:47.656651Z",
     "iopub.status.idle": "2026-01-08T16:52:49.833272Z",
     "shell.execute_reply": "2026-01-08T16:52:49.832988Z"
    },
    "papermill": {
     "duration": 2.182122,
     "end_time": "2026-01-08T16:52:49.834024",
     "exception": false,
     "start_time": "2026-01-08T16:52:47.651902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running BO ."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "print(f\"\\nRunning BO \", end=\"\")\n",
    "\n",
    "state_dict = None\n",
    "# run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "for iteration in range(N_BATCH):\n",
    "\n",
    "    # fit the model\n",
    "    model = get_fitted_model(\n",
    "        train_x=train_x,\n",
    "        train_obj=train_obj,\n",
    "        state_dict=state_dict,\n",
    "    )\n",
    "\n",
    "    # define the qNEI acquisition function\n",
    "    qEI = qExpectedImprovement(\n",
    "        model=model, best_f=train_obj.max()\n",
    "    )\n",
    "\n",
    "    # optimize and get new observation\n",
    "    new_x, new_obj = optimize_acqf_and_get_observation(qEI)\n",
    "\n",
    "    # update training points\n",
    "    train_x = torch.cat((train_x, new_x))\n",
    "    train_obj = torch.cat((train_obj, new_obj))\n",
    "\n",
    "    # update progress\n",
    "    best_value = train_obj.max().item()\n",
    "    best_observed.append(best_value)\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    print(\".\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639b7f7",
   "metadata": {
    "papermill": {
     "duration": 0.009734,
     "end_time": "2026-01-08T16:52:49.860724",
     "exception": false,
     "start_time": "2026-01-08T16:52:49.850990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "EI recommends the best point observed so far. We can visualize what the images corresponding to recommended points *would have* been if the BO process ended at various times. Here, we show the progress of the algorithm by examining the images at 0%, 10%, 25%, 50%, 75%, and 100% completion. The first image is the best image found through the initial random batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb6c0297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T16:52:49.881374Z",
     "iopub.status.busy": "2026-01-08T16:52:49.881162Z",
     "iopub.status.idle": "2026-01-08T16:52:50.031703Z",
     "shell.execute_reply": "2026-01-08T16:52:50.031442Z"
    },
    "papermill": {
     "duration": 0.159136,
     "end_time": "2026-01-08T16:52:50.032587",
     "exception": false,
     "start_time": "2026-01-08T16:52:49.873451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1400 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 6, figsize=(14, 14))\n",
    "percentages = np.array([0, 10, 25, 50, 75, 100], dtype=np.float32)\n",
    "inds = (N_BATCH * BATCH_SIZE * percentages / 100 + 4).astype(int)\n",
    "\n",
    "for i, ax in enumerate(ax.flat):\n",
    "    b = torch.argmax(score_image(decode(train_x[: inds[i], :])), dim=0)\n",
    "    img = decode(train_x[b].view(1, -1)).squeeze().cpu()\n",
    "    ax.imshow(img, alpha=0.8, cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.66949,
   "end_time": "2026-01-08T16:52:50.554718",
   "environment_variables": {},
   "exception": null,
   "input_path": "/Users/saitcakmak/botorch/tutorials/vae_mnist/vae_mnist.ipynb",
   "output_path": "/Users/saitcakmak/botorch/tutorials/vae_mnist/vae_mnist.ipynb",
   "parameters": {},
   "start_time": "2026-01-08T16:52:37.885228",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}